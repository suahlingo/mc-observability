networks:
  internal_network:
    internal: true
  external_network:
    driver: bridge

services:
  mc-o11y-manager:
    image: cloudbaristaorg/mc-observability:edge
    container_name: mc-o11y-service
    restart: always
    ports:
      - 18080:18080
      - 18081:18081
    networks:
      - internal_network
      - external_network
    environment:
      - TARGET_ID=mc-o11y
      - TUMBLEBUG_URL=${TUMBLEBUG_URL:-http://cb-tumblebug:1323}
      - TUMBLEBUG_ID=${TUMBLEBUG_ID:-default}
      - TUMBLEBUG_PW=${TUMBLEBUG_PW:-default}
      - INSIGHT_URL=http://mc-o11y-insight:9001
    volumes:
      - ./manager-conf:/etc/mc-observability-agent/conf:rw
      - /var/log/syslog:/var/log/syslog:ro

  mariadb:
    image: mariadb:10.7.6
    container_name: mc-o11y-maria
    restart: always
    ports:
      - 3306:3306
    volumes:
      - /docker/mariadb/etc/mysql/conf.d:/etc/mysql/conf.d:ro
      - /docker/mariadb/var/lib/mysql:/var/lib/mysql
      - /docker/mariadb/var/log/maria:/var/log/maria
      - ./maria_init.sql:/docker-entrypoint-initdb.d/maria_init.sql
      - ./mariadb-conf/99-max-connections.cnf:/etc/mysql/mariadb.conf.d/99-max-connections.cnf
    environment:
      - TZ="Asia/Seoul"
      - ALLOW_EMPTY_PASSWORD=no
      - MARIADB_ROOT_PASSWORD=qwe1212!Q
      - MARIADB_USER=mc-agent
      - MARIADB_DATABASE=mc_observability
      - MARIADB_PASSWORD=mc-agent
    networks:
      - internal_network
      - external_network

  # CB-Tumblebug
  cb-tumblebug:
    image: cloudbaristaorg/cb-tumblebug:0.9.22
    container_name: cb-tumblebug
    restart: always
    platform: linux/amd64
    networks:
      - internal_network
      - external_network
    ports:
      - 1323:1323
    depends_on:
      - cb-tumblebug-etcd
      # - cb-tumblebug-etcd-conf
#       - cb-spider
    volumes:
      - ./conf/setup.env:/app/conf/setup.env
      - ./conf/cloud_conf.yaml:/app/conf/cloud_conf.yaml
      - ~/.cloud-barista/credentials.yaml.enc.:/app/conf/credentials.yaml.enc
      - /docker/cb-tumblebug-container/meta_db/:/app/meta_db/
      - /docker/cb-tumblebug-container/log/:/app/log/
    environment:
      # - TB_ROOT_PATH=/app
      - TB_SPIDER_REST_URL=http://cb-spider:1024/spider
      - TB_ETCD_ENDPOINTS=http://cb-tumblebug-etcd:2379
      # - TB_ETCD_AUTH_ENABLED=true
      # - TB_ETCD_USERNAME=default
      # - TB_ETCD_PASSWORD=default
      # - TB_SQLITE_URL=localhost:3306
      # - TB_SQLITE_DATABASE=cb_tumblebug
      # - TB_SQLITE_USER=cb_tumblebug
      # - TB_SQLITE_PASSWORD=cb_tumblebug
      # - TB_ALLOW_ORIGINS=*
      # - TB_AUTH_ENABLED=true
      # - TB_API_USERNAME=default
      # - TB_API_PASSWORD=default
      # - TB_AUTOCONTROL_DURATION_MS=10000
      # - TB_SELF_ENDPOINT=localhost:1323
      # - TB_DRAGONFLY_REST_URL=http://cb-dragonfly:9090/dragonfly
      # - TB_DEFAULT_NAMESPACE=default
      # - TB_DEFAULT_CREDENTIALHOLDER=admin
      # - TB_LOGFILE_PATH=/app/log/tumblebug.log
      # - TB_LOGFILE_MAXSIZE=1000
      # - TB_LOGFILE_MAXBACKUPS=3
      # - TB_LOGFILE_MAXAGE=30
      # - TB_LOGFILE_COMPRESS=false
      # - TB_LOGLEVEL=debug
      # - TB_LOGWRITER=both
      # - TB_NODE_ENV=development
    healthcheck: # for CB-Tumblebug
      test: [ "CMD", "curl", "-f", "http://localhost:1323/tumblebug/readyz" ]
      interval: 1m
      timeout: 5s
      retries: 3
      start_period: 10s

  # cb-tumblebug-etcd
  cb-tumblebug-etcd:
    image: gcr.io/etcd-development/etcd:v3.5.14
    container_name: cb-tumblebug-etcd
    restart: always
    networks:
      - internal_network
    ports:
      - 2379:2379
      - 2380:2380
    volumes:
      - /docker/etcd/data:/etcd-data
    entrypoint: /usr/local/bin/etcd
    command:
      - --name
      - s1
      - --data-dir
      - /etcd-data
      - --listen-client-urls
      - http://0.0.0.0:2379
      - --advertise-client-urls
      - http://0.0.0.0:2379
      - --listen-peer-urls
      - http://0.0.0.0:2380
      - --initial-advertise-peer-urls
      - http://0.0.0.0:2380
      - --initial-cluster
      - s1=http://0.0.0.0:2380
      - --initial-cluster-token
      - tkn
      - --initial-cluster-state
      - new
      - --log-level
      - info
      - --logger
      - zap
      - --log-outputs
      - stderr
      - --auth-token
      - simple
    healthcheck: # for etcd
      test: [ "CMD", "/usr/local/bin/etcd", "--version"]
      interval: 1m
      timeout: 5s
      retries: 3
      start_period: 10s

  cb-spider:
    image: cloudbaristaorg/cb-spider_azure_monitoring:edge
    container_name: cb-spider
    restart: always
    platform: linux/amd64
    networks:
      - internal_network
      - external_network
    ports:
      - 1024:1024
    volumes:
      - /docker/cb-spider_azure_monitoring/meta_db/:/root/go/src/github.com/cloud-barista/cb-spider/meta_db/
      - /docker/cb-spider_azure_monitoring/log/:/root/go/src/github.com/cloud-barista/cb-spider/log/
    environment:
      - PLUGIN_SW=OFF
      - SERVER_ADDRESS=localhost
      # if you leave these values empty, REST Auth will be disabled.
      # - API_USERNAME=
      # - API_PASSWORD=
      - SPIDER_LOG_LEVEL=error
      - SPIDER_HISCALL_LOG_LEVEL=error
      - ID_TRANSFORM_MODE=OFF
    healthcheck: # for CB-Spider
      test: [ "CMD", "curl", "-f", "http://localhost:1024/spider/readyz" ]
      interval: 1m
      timeout: 5s
      retries: 3
      start_period: 10s

  influxdb:
    image: influxdb:1.8
    container_name: mc-o11y-influx
    restart: always
    ports:
      - 8086:8086
      - 8082:8082
    environment:
      - INFLUXDB_USER=mc-agent
      - INFLUXDB_PASSWORD=mc-agent
      - INFLUXDB_DB="mc-observability"
    volumes:
      - /docker/influxdb/config:/etc/influxdb
      - /docker/influxdb/var/lib/influxdb:/root/.influxdb
      - ./influxdb_init:/docker-entrypoint-initdb.d
    networks:
      - internal_network
      - external_network

  chronograf:
    image: chronograf:1.9.4
    container_name: mc-o11y-chronograf
    restart: always
    ports:
      - 8888:8888
    volumes:
      - /docker/chronograf_data:/var/lib/chronograf
    networks:
      - internal_network
      - external_network

  kapacitor:
    image: kapacitor:1.7.5
    container_name: mc-o11y-kapacitor
    restart: always
    ports:
      - 9092:9092
    environment:
      - KAPACITOR_INFLUXDB_0_URLS_0=http://mc-o11y-influx:8086
    volumes:
      - /docker/kapacitor_data:/var/lib/kapacitor
    networks:
      - internal_network
      - external_network

  opensearch-node1:
    image: opensearchproject/opensearch:1.3.19
    container_name: opensearch-node1
    restart: always
    environment:
      - cluster.name=opensearch-cluster
      - node.name=opensearch-node1
      - bootstrap.memory_lock=true
      - "OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m"
      - "DISABLE_INSTALL_DEMO_CONFIG=true"
      - "DISABLE_SECURITY_PLUGIN=true"
      - "discovery.type=single-node"
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536
    volumes:
      - /docker/opensearch:/usr/share/opensearch/data
    ports:
      - 9200:9200
      - 9600:9600
    networks:
      - internal_network
      - external_network
        
  opensearch-dashboards:
    image: opensearchproject/opensearch-dashboards:1.3.19
    container_name: opensearch-dashboards
    restart: always
    ports:
      - 5601:5601
    environment:
      - 'OPENSEARCH_HOSTS=["http://opensearch-node1:9200"]'
      - "DISABLE_SECURITY_DASHBOARDS_PLUGIN=true"
    networks:
      - internal_network
      - external_network

  mc-o11y-insight:
    build: ../python/insight
    # image: mc-o11y-insight:0.3.0
    container_name: mc-o11y-insight-service
    restart: always
    ports:
      - 9001:9001
    environment:
      - TZ=Asia/Seoul
    volumes:
      - ../python/insight/log:/mc-insight/log
    networks:
      - internal_network
      - external_network

  mc-o11y-insight-scheduler:
    build: ../python/scheduler
    # image: mc-o11y-insight-scheduler:0.3.0
    container_name: mc-o11y-insight-scheduler
    restart: always
    ports:
      - 9002:9002
    environment:
      - TZ=Asia/Seoul
    networks:
      - internal_network
      - external_network
    volumes:
      - ../python/scheduler/airflow-home:/usr/local/airflow
      - /etc/localtime:/etc/localtime:ro
    command: >
      /bin/bash -c "
        # Wait for MySQL
        sleep 10

        # Clean up pid
        rm -f airflow-webserver.pid

        # Set up metadata database
        airflow db migrate

        # Create default user
        airflow users create --username admin --password admin --email admin@innogrid.com --firstname admin --lastname admin --role Admin

        # Import variables & Make connections
        # airflow variables import -a overwrite /usr/local/airflow/airflow_variables.json
        airflow variables set --description 'O11Y Manger API BASE URL' API_BASE_URL http://mc-o11y-manager:18080/api/o11y

        airflow connections add --conn-type http --conn-host mc-o11y-insight-service --conn-schema http --conn-port 9001 api_base_url
        airflow connections add --conn-type mysql --conn-host mariadb --conn-schema mc_observability --conn-login mc-agent --conn-password mc-agent --conn-port 3306 mcmp_db
        airflow connections add --conn-type http --conn-host mc-manager --conn-port 18080 o11y-manager
        airflow connections add --conn-type influxdb --conn-host mc-o11y-influx --conn-port 8086 --conn-schema downsampling --conn-login mc-agent --conn-password mc-agent influxdb

        # Reload & Run dags
        airflow dags reserialize
        airflow dags unpause anomaly_detection
        airflow dags unpause down_sampling

        # Start airflow
        airflow scheduler & airflow webserver -p 9002

        # Keep the server on no matter what
        sleep infinity
            "
